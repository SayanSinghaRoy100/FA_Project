{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 015043 Project-1"
      ],
      "metadata": {
        "id": "bVjqh2Ee2dkG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XtJ96oNyR3U3",
        "outputId": "849dc79c-067b-4bfc-9a5e-978215b45438"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensorflow Version 2.7.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import nltk \n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "print(\"Tensorflow Version\",tf.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVaBDR3FSjqy"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xX-EZ-qSGDW"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/gdrive/MyDrive/Colab_Notebooks/FA/Project_24thDEC_2021/train_Lstm.csv', encoding = 'latin',header=None)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ct1IX3pJSGGx"
      },
      "outputs": [],
      "source": [
        "df.columns = ['sentiment', 'id', 'date', 'query', 'user_id', 'text']\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUiEf7eaSGLC"
      },
      "outputs": [],
      "source": [
        "# Display multiple outputs from a Cell\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-YKq0UTSGOs"
      },
      "outputs": [],
      "source": [
        "df = df.drop(['id', 'date', 'query', 'user_id'], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGu-IbhE5DTR"
      },
      "source": [
        "# Now we are droping these coloumn to because these are irrelevent for our analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VMsFRPcXEi1"
      },
      "outputs": [],
      "source": [
        "lab_to_sentiment = {0:\"Negative\", 4:\"Positive\"}\n",
        "def label_decoder(label):\n",
        "  return lab_to_sentiment[label]\n",
        "df.sentiment = df.sentiment.apply(lambda x: label_decoder(x))\n",
        "df.head() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Au7uu-wEXErd"
      },
      "outputs": [],
      "source": [
        "val_count = df.sentiment.value_counts()\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.bar(val_count.index, val_count.values)\n",
        "plt.title(\"Sentiment Data Distribution\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0P0r-SRXEwp"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random_idx_list = [random.randint(1,len(df.text)) for i in range(10)] # creates random indexes to choose from dataframe\n",
        "df.loc[random_idx_list,:].head(10) # Returns the rows with the index and display it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XqmrWcoXE0n"
      },
      "outputs": [],
      "source": [
        "stop_words = stopwords.words('english')\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gu_CK3NYSGRh"
      },
      "outputs": [],
      "source": [
        "def preprocess(text, stem=False):\n",
        "  text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n",
        "  tokens = []\n",
        "  for token in text.split():\n",
        "    if token not in stop_words:\n",
        "      if stem:\n",
        "        tokens.append(stemmer.stem(token))\n",
        "      else:\n",
        "        tokens.append(token)\n",
        "  return \" \".join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnPkHccSUlwn"
      },
      "outputs": [],
      "source": [
        "df.text = df.text.apply(lambda x: preprocess(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgeKZqAp3Oel"
      },
      "source": [
        "# Positive Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dvh0KDDUlz4"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "plt.figure(figsize = (20,20)) \n",
        "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.sentiment == 'Positive'].text))\n",
        "plt.imshow(wc , interpolation = 'bilinear')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja7Otqiw3tmJ"
      },
      "source": [
        "# Negative Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZeVnGJTUl3w"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (20,20)) \n",
        "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.sentiment == 'Negative'].text))\n",
        "plt.imshow(wc , interpolation = 'bilinear')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gStfR3Ei4O2L"
      },
      "source": [
        "# Train and test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPqfcBdTUl7X"
      },
      "outputs": [],
      "source": [
        "TRAIN_SIZE = 0.8\n",
        "MAX_NB_WORDS = 100000\n",
        "MAX_SEQUENCE_LENGTH = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxDmjDLlUl-w"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = train_test_split(df, test_size=1-TRAIN_SIZE,\n",
        "                                         random_state=7) # Splits Dataset into Training and Testing set\n",
        "print(\"Train Data size:\", len(train_data))\n",
        "print(\"Test Data size\", len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWeSWsdSUmBo"
      },
      "outputs": [],
      "source": [
        "train_data.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrB-4Qxv42-7"
      },
      "source": [
        "# Tokennizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjHj3grK4ymY"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_data.text)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Vocabulary Size :\", vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcqYr0v04y1z"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "x_train = pad_sequences(tokenizer.texts_to_sequences(train_data.text),\n",
        "                        maxlen = MAX_SEQUENCE_LENGTH)\n",
        "x_test = pad_sequences(tokenizer.texts_to_sequences(test_data.text),\n",
        "                       maxlen = MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "print(\"Training X Shape:\",x_train.shape)\n",
        "print(\"Testing X Shape:\",x_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bzr5SwCm4y_n"
      },
      "outputs": [],
      "source": [
        "labels = train_data.sentiment.unique().tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1d-oojn5qXP"
      },
      "source": [
        "# Lebel encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8q9hpBa4zUg"
      },
      "outputs": [],
      "source": [
        "encoder = LabelEncoder()\n",
        "encoder.fit(train_data.sentiment.to_list())\n",
        "\n",
        "y_train = encoder.transform(train_data.sentiment.to_list())\n",
        "y_test = encoder.transform(test_data.sentiment.to_list())\n",
        "\n",
        "y_train = y_train.reshape(-1,1)\n",
        "y_test = y_test.reshape(-1,1)\n",
        "\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eT7keNl4zbK"
      },
      "outputs": [],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDM77bAK4zef"
      },
      "outputs": [],
      "source": [
        "GLOVE_EMB = '/content/glove.6B.300d.txt'\n",
        "EMBEDDING_DIM = 300\n",
        "LR = 1e-3\n",
        "BATCH_SIZE = 1024\n",
        "EPOCHS = 10\n",
        "MODEL_PATH = '/content/best_model.hdf5'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7Hq3o3S4zhx"
      },
      "outputs": [],
      "source": [
        "embeddings_index = {}\n",
        "\n",
        "f = open(GLOVE_EMB)\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = value = values[0]\n",
        "  coefs = np.asarray(values[1:], dtype='float32')\n",
        "  embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' %len(embeddings_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V21owoPM4zle"
      },
      "outputs": [],
      "source": [
        "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XpnnTgs4zo0"
      },
      "outputs": [],
      "source": [
        "embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                          EMBEDDING_DIM,\n",
        "                                          weights=[embedding_matrix],\n",
        "                                          input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                          trainable=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcKfD0bD8wQ7"
      },
      "source": [
        "# LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLpZ176t8o6R"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Dropout\n",
        "from tensorflow.keras.layers import SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCFZ_cDB8o9z"
      },
      "outputs": [],
      "source": [
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedding_sequences = embedding_layer(sequence_input)\n",
        "x = SpatialDropout1D(0.2)(embedding_sequences)\n",
        "x = Conv1D(64, 5, activation='relu')(x)\n",
        "x = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2))(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "model = tf.keras.Model(sequence_input, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwtSOU2-8pBV"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=LR), loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "ReduceLROnPlateau = ReduceLROnPlateau(factor=0.1,\n",
        "                                     min_lr = 0.01,\n",
        "                                     monitor = 'val_loss',\n",
        "                                     verbose = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXeZcVZM_Snq"
      },
      "outputs": [],
      "source": [
        "print(\"Training on GPU...\") if tf.test.is_gpu_available() else print(\"Training on CPU...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXN-jd2H_Sqy",
        "outputId": "203c4288-bf36-4342-a93d-cd67b6cc2235"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1250/1250 [==============================] - 1385s 1s/step - loss: 0.5195 - accuracy: 0.7388 - val_loss: 0.4826 - val_accuracy: 0.7653 - lr: 0.0010\n",
            "Epoch 2/10\n",
            "1250/1250 [==============================] - 1375s 1s/step - loss: 0.4883 - accuracy: 0.7622 - val_loss: 0.4772 - val_accuracy: 0.7682 - lr: 0.0010\n",
            "Epoch 3/10\n",
            "1250/1250 [==============================] - 1373s 1s/step - loss: 0.4782 - accuracy: 0.7685 - val_loss: 0.4673 - val_accuracy: 0.7751 - lr: 0.0010\n",
            "Epoch 4/10\n",
            "1250/1250 [==============================] - 1382s 1s/step - loss: 0.4718 - accuracy: 0.7724 - val_loss: 0.4640 - val_accuracy: 0.7768 - lr: 0.0010\n",
            "Epoch 5/10\n",
            "1250/1250 [==============================] - 1393s 1s/step - loss: 0.4671 - accuracy: 0.7754 - val_loss: 0.4613 - val_accuracy: 0.7792 - lr: 0.0010\n",
            "Epoch 6/10\n",
            "1250/1250 [==============================] - 1391s 1s/step - loss: 0.4633 - accuracy: 0.7779 - val_loss: 0.4611 - val_accuracy: 0.7796 - lr: 0.0010\n",
            "Epoch 7/10\n",
            "1250/1250 [==============================] - 1384s 1s/step - loss: 0.4605 - accuracy: 0.7796 - val_loss: 0.4587 - val_accuracy: 0.7804 - lr: 0.0010\n",
            "Epoch 8/10\n",
            "1250/1250 [==============================] - 1393s 1s/step - loss: 0.4584 - accuracy: 0.7808 - val_loss: 0.4608 - val_accuracy: 0.7808 - lr: 0.0010\n",
            "Epoch 9/10\n",
            "1250/1250 [==============================] - 1384s 1s/step - loss: 0.4558 - accuracy: 0.7822 - val_loss: 0.4586 - val_accuracy: 0.7816 - lr: 0.0010\n",
            "Epoch 10/10\n",
            "1250/1250 [==============================] - 1364s 1s/step - loss: 0.4546 - accuracy: 0.7829 - val_loss: 0.4573 - val_accuracy: 0.7813 - lr: 0.0010\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
        "                    validation_data=(x_test, y_test), callbacks=[ReduceLROnPlateau])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPAh2USpSJVh"
      },
      "source": [
        "# Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s, (at, al) = plt.subplots(2,1)\n",
        "at.plot(history.history['accuracy'], c= 'b')\n",
        "at.plot(history.history['val_accuracy'], c='r')\n",
        "at.set_title('model accuracy')\n",
        "at.set_ylabel('accuracy')\n",
        "at.set_xlabel('epoch')\n",
        "at.legend(['LSTM_train', 'LSTM_val'], loc='upper left')\n",
        "\n",
        "al.plot(history.history['loss'], c='m')\n",
        "al.plot(history.history['val_loss'], c='c')\n",
        "al.set_title('model loss')\n",
        "al.set_ylabel('loss')\n",
        "al.set_xlabel('epoch')\n",
        "al.legend(['train', 'val'], loc = 'upper left')"
      ],
      "metadata": {
        "id": "T5Fb1ZQMx9Uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iM-gYtir_Sx5"
      },
      "outputs": [],
      "source": [
        "def decode_sentiment(score):\n",
        "    return \"Positive\" if score>0.5 else \"Negative\"\n",
        "\n",
        "\n",
        "scores = model.predict(x_test, verbose=1, batch_size=10000)\n",
        "y_pred_1d = [decode_sentiment(score) for score in scores]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IePV6W_dSbsn"
      },
      "source": [
        "# Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQOoJnGH_S9X"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "\n",
        "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title, fontsize=20)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, fontsize=13)\n",
        "    plt.yticks(tick_marks, classes, fontsize=13)\n",
        "\n",
        "    fmt = '.2f'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label', fontsize=17)\n",
        "    plt.xlabel('Predicted label', fontsize=17)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RG2-bu0YShu_"
      },
      "outputs": [],
      "source": [
        "cnf_matrix = confusion_matrix(test_data.sentiment.to_list(), y_pred_1d)\n",
        "plt.figure(figsize=(6,6))\n",
        "plot_confusion_matrix(cnf_matrix, classes=test_data.sentiment.unique(), title=\"Confusion matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIUzTe1CShys"
      },
      "outputs": [],
      "source": [
        "print(classification_report(list(test_data.sentiment), y_pred_1d))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ch-MFvPCSh2o"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJpbQ9D68pFi"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6B1PH3dS8pJV"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Project-1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}